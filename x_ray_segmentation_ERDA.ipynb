{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99662b4b",
   "metadata": {
    "id": "XiF9uQq63Gw9"
   },
   "source": [
    "# U-Net segmentation example\n",
    "### Advanced Deep Learning 2023\n",
    "This notebook was originally written by Mathias Perslev. It has been changed slightly by Christian Igel and subsequently slightly updated Stefan Sommer (mailto:sommer@di.ku.dk) and Jon Sporring (mailto:sporring@di.ku.dk).\n",
    "\n",
    "We consider the data described in:\n",
    "\n",
    "Bram van Ginneken, Mikkel B. Stegmann, Marco Loog. [Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database](https://doi.org/10.1016/j.media.2005.02.002). *Medical Image Analysis* 10(1): 19-40, 2006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c72803",
   "metadata": {
    "id": "-k7dGxH23GxB"
   },
   "source": [
    "## Installs\n",
    "\n",
    "On non-colab system, is usually good to make an environment and install necessary tools there. E.g., anaconda->jupyter->terminal create an environment, if you have not already, and activate it:\n",
    "```\n",
    "conda create -n adl python=3.9\n",
    "conda activate adl\n",
    "```\n",
    "then install missing packages such as:\n",
    "```\n",
    "conda install ipykernel torch matplotlib torchmetrics scikit-image jpeg\n",
    "conda install -c conda-forge segmentation-models-pytorch ipywidgets\n",
    "```\n",
    "and if you want to add it to jupyter's drop-down menu\n",
    "```\n",
    "ipython kernel install --user --name=adl\n",
    "```\n",
    "Now reload the jupyter-notebook's homepage and make a new or load an existing file. On colab, the tools have to be installed everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f968785",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16226,
     "status": "ok",
     "timestamp": 1681999199749,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "9RnD0QuU86FB",
    "outputId": "12276265-7b6c-4b55-c1c3-d3a62aebef11"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    pass\n",
    "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch\n",
    "\n",
    "    # + [markdown] id=\"_0FulA5_3GxC\"\n",
    "    # ## Imports\n",
    "\n",
    "    # + executionInfo={\"elapsed\": 7291, \"status\": \"ok\", \"timestamp\": 1681999207036, \"user\": {\"displayName\": \"Jon Sporring\", \"userId\": \"14416396696150411278\"}, \"user_tz\": -120} id=\"sp71Um2T3GxC\"\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "from torch import Tensor\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5041b",
   "metadata": {
    "id": "5yTYH2kVkeKK"
   },
   "source": [
    "## Set global device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d4dae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1681999207037,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "95WD78Nu7zYW",
    "outputId": "fc02954d-45fa-43c1-d78d-be4daa9c02e5"
   },
   "outputs": [],
   "source": [
    "# GPU support?\n",
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f73a4",
   "metadata": {
    "id": "isi0fnzy3GxG",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Functions\n",
    "### Loading data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b7c61",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1681999207037,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "Rr5tW2HWZveb",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def load_npz_dataset(\n",
    "    path, keys=(\"x_train\", \"y_train\", \"x_val\", \"y_val\", \"x_test\", \"y_test\")\n",
    "):\n",
    "    archive = np.load(path)\n",
    "    return [archive.get(key) for key in keys]\n",
    "\n",
    "\n",
    "def as_torch_dataset(x_arr, y_arr):\n",
    "    \"\"\"\n",
    "    Takes two numpy arrays of data points and labels (x_arr and y_arr, respectively) and\n",
    "    returns a torch TensorDataset object.\n",
    "\n",
    "    Returns: torch.utils.data.TensorDataset\n",
    "    \"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(x_arr), torch.FloatTensor(y_arr)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94282604",
   "metadata": {
    "id": "l3lNo-Mi86FD",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a47c70",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1681999207037,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "DCz5W51v86FE",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_image_with_segmentation(image, segmentation, ax=None):\n",
    "    \"\"\"\n",
    "    Plots an image with overlayed segmentation mask. Segmentation is thresholded at 0.5\n",
    "\n",
    "    Returns: plt.fig and ax objects\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    im = image.cpu().detach().numpy()\n",
    "    segm = segmentation.cpu().detach().numpy()\n",
    "    ax.imshow(im.squeeze(), cmap=\"gray\")\n",
    "    mask = segm < 0.5\n",
    "    ax.imshow(mask.squeeze(), cmap=\"Set1\", alpha=0.5)\n",
    "    return plt.gcf(), ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294bab01",
   "metadata": {
    "id": "XJtsklqj3GxH",
    "lines_to_next_cell": 2
   },
   "source": [
    "### EvaluatingÂ a model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9d311",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1681999207038,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "9t46YrIk86FE"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_single_image(model, x, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a single data point on the device.\n",
    "\n",
    "    Returns: model(x)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        return model(x.view(1, *x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277f07b",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1681999207038,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "ZzBstTq43GxH",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, metrics_dict, reduction=True, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate a model 'model' on all batches of a torch DataLoader 'data_loader'.\n",
    "    On each batch, compute all metric functions stored in dictionary 'metrics_dict'.\n",
    "\n",
    "    Returns: dict of metric_name: (list of batch-wise metrics if reduction == False, else single scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # defaultdict(list) returns a dictionary-like object with default_factory list.\n",
    "    # When a new key is encountered, an entry is automatically created of type default_factory.\n",
    "    metrics = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y) in enumerate(data_loader):\n",
    "            # Predict on batch\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            logits = model(batch_x)\n",
    "\n",
    "            # Compute all metrics\n",
    "            for metric_name, metric_func in metrics_dict.items():\n",
    "                value = metric_func(\n",
    "                    logits.cpu(), batch_y.cpu()\n",
    "                ).item()  # .cpu().numpy()\n",
    "                metrics[metric_name].append(value)\n",
    "\n",
    "    if reduction == True:\n",
    "        # Return mean values\n",
    "        return {key: np.mean(value) for key, value in metrics.items()}\n",
    "    else:\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebed75e",
   "metadata": {
    "id": "-cSZX0bq86FF",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Saving and loading model and optimizer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857204b3",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1681999207038,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "4jOELTgQ86FF",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def save_model(model, path, optimizer=None):\n",
    "    \"\"\"\n",
    "    Saves the state_dict of a torch model and optional optimizer to 'path'\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    state = {\"model\": model.state_dict()}\n",
    "    if optimizer is not None:\n",
    "        state[\"optimizer\"] = optimizer.state_dict()\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_model(model, path, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loads the state_dict of a torch model and optional optimizer from 'path'\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    state = torch.load(path)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9df5e6",
   "metadata": {
    "id": "E23GGXNl86FF",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Plotting training/validation histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8c10d",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1681999207039,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "dwdENGGQ86FF",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_histories(train_history=None, val_history=None, label=\"Loss\"):\n",
    "    \"\"\"\n",
    "    Takes a list of training and/or validation metrics and plots them\n",
    "    Returns: plt.figure and ax objects\n",
    "    \"\"\"\n",
    "    if not train_history and not val_history:\n",
    "        raise ValueError(\n",
    "            \"Must specify at least one of 'train_histories' and 'val_histories'\"\n",
    "        )\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    epochs = np.arange(len(train_history or val_history))\n",
    "    if train_history:\n",
    "        ax.plot(epochs, train_history, label=\"Training\", color=\"black\")\n",
    "    if val_history:\n",
    "        ax.plot(epochs, val_history, label=\"Validation\", color=\"darkred\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(label)\n",
    "    ax.legend(loc=0)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee3066",
   "metadata": {
    "id": "kPW7_p1c3GxH",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Main training loop\n",
    "\n",
    "We want to track the F1 score during training. This generates some additional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fcff1a",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1681999207039,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "Zpq0Qknp3GxH"
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(\n",
    "    model,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs,\n",
    "    metrics_dict,\n",
    "    device=device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run 1 epoch of training\n",
    "    Changes to model parameters and optimizer occour internally (state updates)\n",
    "    Returns:\n",
    "        two dictionaries, training and a validation metrics\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        # Zero out stored gradients for all parameters\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"   -- Batch {i+1}/{len(train_loader)}\", end=\" / \")\n",
    "        # Predict on batch\n",
    "        logits = model(batch_x)\n",
    "\n",
    "        # Compute loss function\n",
    "        loss_tensor = loss(logits, batch_y)\n",
    "        loss_scalar = loss_tensor.detach().cpu().numpy()\n",
    "        train_losses.append(loss_scalar)\n",
    "        print(\"Loss: \", loss_scalar)\n",
    "\n",
    "        # Backprop and step\n",
    "        loss_tensor.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Run validation\n",
    "    print(\"   Validation running...\")\n",
    "    val_metrics = evaluate_model(\n",
    "        model=model, data_loader=val_loader, metrics_dict=metrics_dict\n",
    "    )\n",
    "    # Return loss and metrics as dicts\n",
    "    return {\"loss\": np.mean(train_losses)}, val_metrics\n",
    "\n",
    "\n",
    "def merge_list_of_dicts(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Takes a list of dictionaries and merges them into a single dictionary pointing to lists\n",
    "\n",
    "    E.g. [{\"loss\": 5}, {\"loss\": 3}, {\"loss\": -2, \"F1\": 0.5}] --> {\"loss\": [5, 3, -2], \"F1\": [0.5]}\n",
    "\n",
    "    Returns: dict\n",
    "    \"\"\"\n",
    "    merged = defaultdict(list)\n",
    "    for dict_ in list_of_dicts:\n",
    "        for value, key in dict_.items():\n",
    "            merged[value].append(key)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    model,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs,\n",
    "    init_epoch=None,\n",
    "    metrics_dict=None,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training of a model given a loss function, optimizer and a set of training and validation data.\n",
    "    Supports computing additional metrics on the validation set (only) via the metrics_dict param.\n",
    "    Specify save_path to store the model at each epoch.\n",
    "\n",
    "    Returns:\n",
    "        Two lists of metric dictionaries for each epoch for training and validation, specifically\n",
    "    \"\"\"\n",
    "    train_history, val_history = [], []\n",
    "\n",
    "    metrics_with_loss = {\"loss\": loss}\n",
    "    if metrics_dict is not None:\n",
    "        metrics_with_loss.update(metrics_dict)\n",
    "\n",
    "    if init_epoch == None:\n",
    "        init_epoch = 0\n",
    "    try:\n",
    "        for i in range(init_epoch, n_epochs):\n",
    "            print(f\"Epoch {i+1}/{n_epochs}\")\n",
    "            train_metrics, val_metrics = run_one_epoch(\n",
    "                model=model,\n",
    "                loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                n_epochs=n_epochs,\n",
    "                metrics_dict=metrics_with_loss,\n",
    "            )\n",
    "            wandb.log(\n",
    "                {\"epoch\": i + 1, \"train_loss\": train_metrics, \"val_loss\": val_metrics}\n",
    "            )\n",
    "            print(\"   Mean epoch metrics:\")\n",
    "            print(f\"   Training:   {pformat(train_metrics)}\")\n",
    "            print(f\"   Validation: {pformat(val_metrics)}\")\n",
    "            train_history.append(train_metrics), val_history.append(val_metrics)\n",
    "            wandb.watch(model, loss, log=\"all\")\n",
    "\n",
    "            if save_path:\n",
    "                save_path_epoch = f\"epoch_{i+1}_{save_path}\"\n",
    "                print(f\"   Saving to: {save_path_epoch}\")\n",
    "                save_model(model, save_path_epoch, optimizer)\n",
    "                old_path_epoch = f\"epoch_{i}_{save_path}\"\n",
    "                if os.path.exists(old_path_epoch):\n",
    "                    os.remove(old_path_epoch)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training stopped.\")\n",
    "        pass\n",
    "\n",
    "    # Merge list of training and validation dicts into single dicts\n",
    "    return merge_list_of_dicts(train_history), merge_list_of_dicts(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273c621",
   "metadata": {
    "id": "5vzrPKEQWBGc"
   },
   "source": [
    "## Main program\n",
    "### Mount Google drive when using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3f11b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41590,
     "status": "ok",
     "timestamp": 1681999248620,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "5GoilmVm3Pts",
    "outputId": "ef4a8a89-f36c-4c7a-cf14-c5263815b7ee"
   },
   "outputs": [],
   "source": [
    "# If you are working from Colab, better mount your google drive and change directory appropriately.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/gdrive/\")\n",
    "    os.chdir(\"gdrive/MyDrive/Colab Notebooks/ADL/assignment1\")\n",
    "except:\n",
    "    print(\"Google drive not mounted\")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d70b44",
   "metadata": {
    "id": "oII1ZIcT3GxE"
   },
   "source": [
    "### Setup database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ead3f",
   "metadata": {
    "id": "NHFjgYdp3GxE"
   },
   "source": [
    "Load database with chest X-rays with lung segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799bcde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "executionInfo": {
     "elapsed": 3286,
     "status": "ok",
     "timestamp": 1681999251895,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "hM393wDKXae-",
    "outputId": "f0a7bf9d-e90e-47d2-8a9e-85051b8e73eb"
   },
   "outputs": [],
   "source": [
    "data_root = \"./datasets\"\n",
    "data_npz = \"lung_field_dataset.npz\"\n",
    "data_fn = os.path.join(data_root, \"lung_field_dataset.npz\")\n",
    "force_download = False\n",
    "\n",
    "if (not os.path.exists(data_fn)) or force_download:\n",
    "    download_url(\"https://sid.erda.dk/share_redirect/gCTc6o3KAh\", data_root, data_npz)\n",
    "else:\n",
    "    print(\"Using existing\", data_fn)\n",
    "\n",
    "# Load train/val/test data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_npz_dataset(data_fn)\n",
    "# Bring images into PyTorch format\n",
    "x_train = np.moveaxis(x_train, 3, 1)\n",
    "y_train = np.moveaxis(y_train, 3, 1)\n",
    "x_val = np.moveaxis(x_val, 3, 1)\n",
    "y_val = np.moveaxis(y_val, 3, 1)\n",
    "x_test = np.moveaxis(x_test, 3, 1)\n",
    "y_test = np.moveaxis(y_test, 3, 1)\n",
    "\n",
    "print(\"x train:\", x_train.shape)\n",
    "print(\"y train:\", y_train.shape)\n",
    "print(\"x val: \", x_val.shape)\n",
    "print(\"y val: \", y_val.shape)\n",
    "print(\"x test:\", x_test.shape)\n",
    "print(\"y test:\", y_test.shape)\n",
    "\n",
    "# Init torch datasets\n",
    "train_dataset = as_torch_dataset(x_train, y_train)\n",
    "val_dataset = as_torch_dataset(x_val, y_val)\n",
    "test_dataset = as_torch_dataset(x_test, y_test)\n",
    "\n",
    "# Init dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Plot an example\n",
    "fig, ax = plot_image_with_segmentation(*train_dataset[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3bef0",
   "metadata": {
    "id": "r8z2VvB83GxI"
   },
   "source": [
    "### Init model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac639c93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4411,
     "status": "ok",
     "timestamp": 1681999256297,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "Xjrw4VsE3GxI",
    "outputId": "8a6e031b-c944-438f-9fea-3c0e7c43b5f8"
   },
   "outputs": [],
   "source": [
    "# Init U-Net model\n",
    "model = smp.Unet(encoder_name=\"efficientnet-b0\", in_channels=1, classes=1)\n",
    "\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0bcc03",
   "metadata": {
    "id": "7nXGVTLw3GxI"
   },
   "source": [
    "### Continue training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474aa73",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1681999256299,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "6b2g5WUq3GxI"
   },
   "outputs": [],
   "source": [
    "# Specify integer, starting at 1\n",
    "init_epoch = None\n",
    "if init_epoch != None:\n",
    "    load_model(model, f\"epoch_{init_epoch}_model.ckpt\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da29bcf",
   "metadata": {
    "id": "NrxO77vW3GxI"
   },
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7621f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223042,
     "status": "ok",
     "timestamp": 1681999479331,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "LWETbe5d3GxJ",
    "lines_to_next_cell": 2,
    "outputId": "101d4229-320e-45f9-80e0-5b07078204ef"
   },
   "outputs": [],
   "source": [
    "# Define loss and metrics\n",
    "loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "metrics = {\n",
    "    \"f1\": torchmetrics.classification.F1Score(\n",
    "        task=\"binary\", num_classes=1, average=\"macro\", mdmc_average=\"samplewise\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run training\n",
    "sweep_config = {\n",
    "    \"name\": \"final\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"val_loss.f1\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"distribution\": \"q_log_uniform_values\", \"max\": 256, \"min\": 32},\n",
    "        \"epochs\": {\"distribution\": \"int_uniform\", \"max\": 250, \"min\": 100},\n",
    "        \"learning_rate\": {\"distribution\": \"uniform\", \"max\": 0.1, \"min\": 0},\n",
    "        \"loss_function\": {\"values\": [\"mse\", \"huber\", \"mae\"]},\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]},\n",
    "    },\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"ku_adl_x_ray\")\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(project=\"ku_adl_x_ray\", config=config, resume=True):\n",
    "        config = wandb.config\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=config.batch_size, shuffle=False\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=1, shuffle=False\n",
    "        )\n",
    "\n",
    "        if config.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        elif config.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer function `{config.optimizer}`\")\n",
    "\n",
    "        if config.loss_function == \"mse\":\n",
    "            loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        elif config.loss_function == \"mae\":\n",
    "            loss = torch.nn.L1Loss(reduction=\"mean\")\n",
    "        elif config.loss_function == \"huber\":\n",
    "            loss = torch.nn.HuberLoss(reduction=\"mean\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function `{config.loss_function}`\")\n",
    "        train_history, val_history = training_loop(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            init_epoch=None,\n",
    "            n_epochs=config.epochs,\n",
    "            metrics_dict=metrics,\n",
    "            # save_path=\"model.ckpt\",\n",
    "        )\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b37cb5",
   "metadata": {
    "id": "4WHHZc5K3GxJ"
   },
   "source": [
    "### Plot training and validation histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"ku_adl_x_ray\")\n",
    "model = smp.Unet(encoder_name=\"efficientnet-b0\", in_channels=1, classes=1)\n",
    "model.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "train_history, val_history = training_loop(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    init_epoch=None,\n",
    "    n_epochs=100,\n",
    "    metrics_dict=metrics,\n",
    "    save_path=\"model.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c015e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1681999479332,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "rxBzn18s3GxJ",
    "outputId": "0a0ef048-dcb0-45bf-de5a-09fde57c57c9"
   },
   "outputs": [],
   "source": [
    "plot_histories(train_history[\"loss\"], val_history[\"loss\"], label=\"Loss\")\n",
    "plot_histories(train_history=None, val_history=val_history[\"f1\"], label=\"F1 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67476ae9",
   "metadata": {
    "id": "PFD4Yrxy3GxJ"
   },
   "source": [
    "### Evaluate on single test-set image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3942c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "executionInfo": {
     "elapsed": 1497,
     "status": "ok",
     "timestamp": 1681999480819,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "Ncio8D4S3GxJ",
    "outputId": "ca8c728d-58bd-4b50-d41b-48572dac5248"
   },
   "outputs": [],
   "source": [
    "# Select a test image\n",
    "x, y = test_dataset[0]\n",
    "pred = evaluate_model_on_single_image(model, x)\n",
    "\n",
    "# Plot the result\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 15))\n",
    "ax1.set_title(\"True mask\")\n",
    "ax2.set_title(\"Predicted mask\")\n",
    "plot_image_with_segmentation(x, y, ax=ax1)\n",
    "plot_image_with_segmentation(x, pred, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f35f11",
   "metadata": {
    "id": "qbWED8IJ3GxJ"
   },
   "source": [
    "### Evaluate on whole test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd67d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1676,
     "status": "ok",
     "timestamp": 1681999482489,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "jgPqlai_3GxK",
    "outputId": "4ce332a3-2b72-4ebc-c59e-74f6d5524011"
   },
   "outputs": [],
   "source": [
    "# OBS: Returns batch-wise metrics, but test_loader has batch_size = 1\n",
    "f1_test_scores = evaluate_model(model, test_loader, metrics, reduction=False)[\"f1\"]\n",
    "\n",
    "print(\"Test cases:\", len(f1_test_scores))\n",
    "print(\"Mean F1:   \", np.mean(f1_test_scores))\n",
    "print(\"STD  F1:   \", np.std(f1_test_scores))\n",
    "print(\"Min. F1:   \", np.min(f1_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afadea5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1681999626786,
     "user": {
      "displayName": "Jon Sporring",
      "userId": "14416396696150411278"
     },
     "user_tz": -120
    },
    "id": "0qZqkTsF86FJ",
    "outputId": "2dbc1c31-e4df-4dba-ab42-7c5120dcae31"
   },
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07999449",
   "metadata": {
    "id": "Qt5Abwsl_XvG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "adl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
